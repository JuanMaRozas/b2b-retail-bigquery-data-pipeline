# ðŸ›’ B2B Retail Data Pipeline on Google BigQuery

## Project Overview

This project automates the ingestion, processing, and governance of B2B retail data into Google BigQuery.  
It simulates a real-world scenario where multiple data sources â€” such as sales, stock levels, and product loss â€” must be securely managed, anonymized, and made available for analytics.

Key Features:
- âœ… Incremental CSV uploads
- âœ… Load date tracking for batch auditing and historical analysis
- âœ… Data masking for sensitive product and store fields
- âœ… Looker Studio dashboard connection for data visualization
- âœ… Production-ready scripts structured for scalability and governance

This project demonstrates critical skills for Data Management leadership, including secure ingestion pipelines, cloud governance, and analytics enablement.

---

## Architecture Overview

- **Data Source**: Weekly B2B retail CSV files (Sales, Stock, Product Loss)
- **Data Ingestion**: Python scripts using BigQuery Client API
- **Data Storage**: Google BigQuery (GCP)
- **Security & Governance**:
  - Service Account authentication
  - Explicit schema enforcement
  - Load date tracking on ingestion
  - Data Anonymization (SHA256 & Sequential Labeling)
- **Visualization**: Google Looker Studio connected to BigQuery masked views

---

## Project Structure

```plaintext
b2b-retail-bigquery-data-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ upload_sales_to_bq.py
â”‚   â”œâ”€â”€ upload_product_loss_to_bq.py
â”‚   â”œâ”€â”€ upload_stock_to_bq.py
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ add_load_date_existing_tables.sql
â”‚   â””â”€â”€ masked_views/
â”‚       â”œâ”€â”€ create_stock_masked_view.sql
â”‚       â”œâ”€â”€ create_sales_masked_view.sql
â”‚       â””â”€â”€ create_product_loss_masked_view.sql
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ dashboard_overview.png
â”œâ”€â”€ sample_data/
â”‚   â”œâ”€â”€ sales_sample.csv
â”‚   â”œâ”€â”€ product_loss_sample.csv
â”‚   â”œâ”€â”€ stock_sample.csv

---

## How to Run the Project

1. Prerequisites
- A Google Cloud Platform account
- BigQuery dataset created (e.g., tivoli_dataset)
- Service Account credentials with BigQuery permissions

2. Setup Environment
pip install -r requirements.txt
